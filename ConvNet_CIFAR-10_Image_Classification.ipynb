{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, I'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. I'll preprocess the images, then train a convolutional neural network on all the samples. The images will be normalized and the labels one-hot encoded.  I'll apply a convolutional net, max pooling, dropout, and fully connected layers.  At the end, I'll test the neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 27:\n",
      "Image - Min Value: 18 Max Value: 236\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0Bhih4ACvt/aurVKoau2vYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11678ba20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 27\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below the `normalize` function will take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    min-max function: X' = a + ( ((x- Xmin)/(b-a))  /  (Xmax - Xmin) )\n",
    "    \"\"\"\n",
    "    b = 1.0\n",
    "    a = 0.0\n",
    "    xMax = np.max(x)\n",
    "    xMin = np.min(x)\n",
    "    return (a + (((x - xMin)/(b-a)) / (xMax - xMin)) )\n",
    "\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, this is a function for preprocessing.  The `one_hot_encode` function will take input, `x`, are a list of labels.  The function will return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Will also save the map of encodings outside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    return np.eye(10)[x]\n",
    "    \"\"\"\n",
    "    labels = np.array(x)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    return lb.transform(labels)\n",
    "\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again. (To do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is the first checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** Quick access to Tensor API - [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers.\n",
    "\n",
    ">Tensorflow `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d).\n",
    "\n",
    ">Tensorflow Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. \n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # Completed: Implement Function\n",
    "    height, width, depth = image_shape\n",
    "    return tf.placeholder(tf.float32, shape=[None, height, width, depth], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # Completed: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # Completed: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, I implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * Use 'SAME' padding\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * Use 'SAME' padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #1 Create the variables for the weights and the bias\n",
    "    input_depth = x_tensor.shape[3].value\n",
    "    #input_depth = x_tensor.get_shape().as_list()[3]\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], input_depth, \n",
    "                                               conv_num_outputs], mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    #2 Convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    #3 Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    #4 Activation layer\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #5 Convolution #2\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    #6 Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    #7 Activation layer\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #8 Max Pooling\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                           strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "The `flatten` function changes the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "     # Alternate solution - return tf.contrib.layers.flatten(x_tensor)\n",
    "        \n",
    "        \n",
    "    f_height, f_width, f_depth = x_tensor.get_shape().as_list()[1:]\n",
    "    flat_size = f_height * f_width * f_depth\n",
    "    return tf.reshape(x_tensor, [-1, flat_size])\n",
    "    \n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement a `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #1 Weights and bias\n",
    "    batch_size = x_tensor.shape[1].value\n",
    "    #batch_size = x_tensor.get_shape().as_list()[-1]\n",
    "    weights = tf.Variable(tf.truncated_normal([batch_size, num_outputs]), name=\"weights_fully_connected\")\n",
    "    bias = tf.Variable(tf.zeros(num_outputs), name=\"bias_fully_connected\")\n",
    "    \n",
    "    #2 Fully connected layer\n",
    "    fc_layer = tf.nn.bias_add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    #3 Activation\n",
    "    return tf.nn.relu(fc_layer)\n",
    "\n",
    "\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*).\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #1 Weights and bias\n",
    "    batch_size = x_tensor.shape[1].value\n",
    "    #batch_size = x_tensor.get_shape().as_list()[-1]  \n",
    "    weights = tf.Variable(tf.truncated_normal([batch_size, num_outputs], mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    #2 Fully connected layer\n",
    "    layer_fully_connected = tf.nn.bias_add(tf.matmul(x_tensor, weights), bias)\n",
    "    return layer_fully_connected\n",
    "\n",
    "\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Convolutional Model\n",
    "Implement function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    # conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, \n",
    "    #                          conv_strides, pool_ksize, pool_strides)\n",
    "    conv_outputs = [32, 64, 192]\n",
    "    conv_ksize = [[3, 3], [6,6]]\n",
    "    conv_strides = [1, 1]\n",
    "    pool_ksize = [[3, 3], [2,2]]\n",
    "    pool_strides = [2, 2]\n",
    "    cnvNet_1 = conv2d_maxpool(x, conv_outputs[0], conv_ksize[0], conv_strides, pool_ksize[0], pool_strides)\n",
    "    cnvNet_2 = conv2d_maxpool(cnvNet_1, conv_outputs[1], conv_ksize[0], conv_strides, pool_ksize[1], pool_strides)\n",
    "    cnvNet_3 = conv2d_maxpool(cnvNet_2, conv_outputs[2], conv_ksize[1], conv_strides, pool_ksize[1], pool_strides)\n",
    "    \n",
    "    # Apply a Flatten Layer\n",
    "    cnvNet_3 = flatten(cnvNet_3)\n",
    "    \n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    # Play around with different number of outputs\n",
    "    fc_outputs = [256, 512, 64]\n",
    "    fconn_1 = fully_conn(cnvNet_3, fc_outputs[0])\n",
    "    fconn_1 = tf.nn.dropout(fconn_1, keep_prob)\n",
    "    \n",
    "    fconn_2 = fully_conn(fconn_1, fc_outputs[1])\n",
    "    fconn_2 = tf.nn.dropout(fconn_2, keep_prob)\n",
    "    \n",
    "    fconn_2 = fully_conn(fconn_1, fc_outputs[2])\n",
    "    fconn_2 = tf.nn.dropout(fconn_2, keep_prob)\n",
    "           \n",
    "    # Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    cnvNet_output = output(fconn_1, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return cnvNet_output\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # Completed: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, \n",
    "                             keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # Completed: Implement Function\n",
    "    cost = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.5f}'.format(cost, accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "keep_probability = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3276 Validation Accuracy: 0.22180\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.6663 Validation Accuracy: 0.32640\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.4297 Validation Accuracy: 0.38000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.1783 Validation Accuracy: 0.42080\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.9775 Validation Accuracy: 0.44020\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.8246 Validation Accuracy: 0.46340\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.6618 Validation Accuracy: 0.47080\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.5657 Validation Accuracy: 0.46980\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.4616 Validation Accuracy: 0.48580\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.3690 Validation Accuracy: 0.49220\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.2986 Validation Accuracy: 0.49300\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.2737 Validation Accuracy: 0.49800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.2219 Validation Accuracy: 0.51040\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.1572 Validation Accuracy: 0.51500\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.1707 Validation Accuracy: 0.52700\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.1471 Validation Accuracy: 0.51600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.1771 Validation Accuracy: 0.50860\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.1390 Validation Accuracy: 0.51460\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.1156 Validation Accuracy: 0.50860\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.0976 Validation Accuracy: 0.50800\n"
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "### Why 50-80% Accuracy?\n",
    "50% isn't bad for a simple CNN.  Pure guessing would get 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130). \n",
    "\n",
    "##### I will keep training the network to get closer to 90%!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
